{"cells":[{"cell_type":"markdown","metadata":{"id":"h1wWTKJIZwNn"},"source":["## Homophobic lyrics detection - Binary Classification"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17751,"status":"ok","timestamp":1716836551582,"user":{"displayName":"Sergio Damian","userId":"14141393108500213286"},"user_tz":360},"id":"4jKe6Z88t8cS","outputId":"d874ed6d-d07a-485b-edba-b2081511506a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0e9ZYbX_Zdb"},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/HOMO-MEX/task_3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mju2plfz-9Ol"},"outputs":[],"source":["from config import config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kz1-GDwGWKet"},"outputs":[],"source":["%%capture\n","#! pip install --quiet \"torchvision\" \"torch>==1.10\" \"pytorch-lightning>=1.3\" \"torchmetrics>=0.3\" \"typing-extensions<4,>=3.7.4.3\" \"tf-estimator-nightly==2.8.0.dev2021122109\" \"folium==0.2.1\"\n","! pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5u7HouB5KAH"},"outputs":[],"source":["%%capture\n","!pip install transformers # pre-trained models from https://huggingface.co/"]},{"cell_type":"markdown","metadata":{"id":"bstJaTW7bkVI"},"source":["### Import all the libraries and functions we will use"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YZaDj3XPY79"},"outputs":[],"source":["import re\n","import os\n","import time\n","import datetime\n","import random\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","from transformers import BertModel\n","from transformers import BertTokenizerFast\n","from transformers import BertForSequenceClassification\n","\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader, SequentialSampler\n","\n","from torchmetrics.classification import MulticlassF1Score, BinaryF1Score\n","\n","pd.set_option('display.max_colwidth',None)"]},{"cell_type":"markdown","metadata":{"id":"Kfv-X7UtcIS6"},"source":["### Environment set up\n"]},{"cell_type":"markdown","metadata":{"id":"uR_76GQMaWO7"},"source":["Select the device where the model is to be trained."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716836654489,"user":{"displayName":"Sergio Damian","userId":"14141393108500213286"},"user_tz":360},"id":"MWmfcRibcSEa","outputId":"80706102-347b-4122-a73d-2b20fe7ca0fa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":7}],"source":["run_on = 'cuda' if torch.cuda.is_available() else 'cpu'\n","DEVICE = torch.device(run_on)\n","DEVICE"]},{"cell_type":"markdown","metadata":{"id":"7cJHIgIpPY8X"},"source":["Set a random seed to ensure that the results are reproducible when attempting to run this *notebook* again."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ucyAVWNSPY8c"},"outputs":[],"source":["# set random seed\n","def set_seed(value):\n","    random.seed(value)\n","    np.random.seed(value)\n","    torch.manual_seed(value)\n","    torch.cuda.manual_seed_all(value)\n","\n","set_seed(config.RANDOM_STATE)"]},{"cell_type":"markdown","metadata":{"id":"9yQwYzSucWXF"},"source":["### Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6vZ9M6OneuC"},"outputs":[],"source":["class CustomDataset:\n","  def __init__(self, datapath, version, batch_size, modelname, test_size=0.2):\n","    self.datapath = datapath\n","    self.batch_size = batch_size\n","    self.dataloaders = {}\n","    self.dfs = {}\n","    self.tokenizer = BertTokenizerFast.from_pretrained(modelname\n","                                          ,do_lower_case=(True if 'uncased' in modelname else False))\n","    self.prepareData(version)\n","\n","  def prepareData_kfold(self):\n","    df = pd.read_csv(self.datapath, converters={ config.TEXT:str, config.TARGET:str })\n","    df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n","    df[\"label\"] = df[\"label\"].map({ \"P\": 0, \"NP\": 1 })\n","\n","    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=config.RANDOM_STATE)\n","    kfolds = skf.split(df[config.TEXT], df[config.TARGET])\n","\n","    for i, (train_index, test_index) in enumerate(kfolds):\n","      train = df.iloc[list(train_index)]\n","      test = df.iloc[list(test_index)]\n","\n","      train = train.sample( len(train), random_state=config.RANDOM_STATE )\n","      test = test.sample( len(test), random_state=config.RANDOM_STATE )\n","\n","      self.dfs[i] = { \"train\": train, \"val\": test }\n","      self.dataloaders[i] = { \"train\": self.getTensorDataset(train), \"val\": self.getTensorDataset(test) }\n","\n","    return self.dataloaders\n","\n","  def prepareData(self, version):\n","    dataset_names = [ \"train\", \"dev\", \"test\" ]\n","    for key in dataset_names:\n","      df = pd.read_csv(f\"{self.datapath}{key}_{version}.csv\", converters={ config.TEXT:str })\n","      if key == \"train\":\n","        # class1 = df[ df[config.TARGET] == \"P\"]\n","        # class2 = df[ df[config.TARGET] == \"NP\"].sample(len(class1), random_state=config.RANDOM_STATE)\n","        # df = pd.concat([ class1, class2 ])\n","        df = df.sample( len(df), random_state=config.RANDOM_STATE )\n","\n","      inputs, masks = self.tokenize(df[\"text\"])\n","\n","      if config.TARGET in list(df.columns):\n","        df[config.TARGET] = df[config.TARGET].map({ \"P\": 0, \"NP\": 1 })\n","        labels = torch.tensor(df[config.TARGET].to_numpy(), dtype=torch.long)\n","        data = TensorDataset(inputs, masks, labels)\n","      else:\n","        df[config.ID] = df[config.ID].apply(lambda x: int(x.replace(\"_Track3\", \"\")))\n","        ids = torch.tensor(df[config.ID].to_numpy(), dtype=torch.long)\n","        data = TensorDataset(inputs, masks, ids)\n","\n","      sampler = SequentialSampler(data)\n","      self.dfs[key] = df\n","      self.dataloaders[key] = DataLoader(data,\n","                                         sampler=sampler,\n","                                         batch_size=self.batch_size,\n","                                         num_workers=0)\n","      print(\"...dataloader for\", key, \"completed\")\n","    return self.dataloaders\n","\n","  def getTensorDataset(self, dataset):\n","    inputs, masks = self.tokenize(dataset[config.TEXT])\n","    labels = torch.tensor(dataset[config.TARGET].to_numpy(), dtype=torch.float32)\n","    data = TensorDataset(inputs, masks, labels)\n","    sampler = SequentialSampler(data)\n","    dataloader = DataLoader(data, sampler=sampler, batch_size=self.batch_size, num_workers=0)\n","    return dataloader\n","\n","  def tokenize(self, dataset):\n","    input_ids = []\n","    attention_mask = []\n","\n","    for doc in dataset:\n","      encoded_doc = self.tokenizer.encode_plus(doc,\n","                                          add_special_tokens=True,\n","                                          max_length=300,\n","                                          truncation=True,\n","                                          padding=\"max_length\",\n","                                          return_token_type_ids=False\n","                                          )\n","\n","      input_ids.append(encoded_doc['input_ids'])\n","      attention_mask.append(encoded_doc['attention_mask'])\n","\n","    return (torch.tensor(input_ids), torch.tensor(attention_mask))"]},{"cell_type":"markdown","metadata":{"id":"yjxzchnTfs_N"},"source":["### Training\n","\n","This process consists of transforming the words of the messages into features expected by BETO.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UkS1XgJF_dT"},"outputs":[],"source":["# PARAMETERS\n","import math\n","\n","# Model\n","NUM_LABELS = 2\n","TARGET_NAMES = [\"P\", \"NP\"]\n","\n","# Training\n","BATCH_SIZE = 8\n","GRADIENT_ACCUMULATOR_SIZE = 1\n","NUM_EPOCHS = 10\n","\n","GET_ATTENTIONS = False\n","GET_HIDDEN_STATES = False\n","DROPOUT = 0.01\n","\n","# Optimizer\n","LR = 2e-5\n","EPS = 1e-10\n","WEIGHT_DECAY = 0.01\n","BETAS = (0.9, 0.999)\n","AMSGRAD = False\n","\n","# Loss\n","REDUCTION = \"sum\"\n","LABEL_SMOOTHING = 0\n","\n","# Scheduler\n","POWER = 1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mfsStD_9wlZ8"},"outputs":[],"source":["#fuction to format time\n","def format_time(elapsed):\n","  elapsed_rounded = int(round((elapsed)))\n","  return str(datetime.timedelta(seconds=elapsed_rounded))\n","\n","#function to clean cuda memory\n","def clean_cuda_memory(iterable_var):\n","  for elem in iterable_var:\n","    elem.to('cpu')\n","    del elem\n","  torch.cuda.empty_cache()\n","  gc.collect()\n","\n","def get_class_weights(classes_list, labels):\n","  return compute_class_weight('balanced', classes=classes_list, y=labels)\n","\n","#variable to plot the confusion matrix\n","conf = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JfhU2Zm0-8q7"},"outputs":[],"source":["class TextClassifier(nn.Module):\n","    def __init__(self, modelpath, freeze_model=False, num_labels=2, get_att=False, get_hs=False, dropout=0.05):\n","        super(TextClassifier, self).__init__()\n","        self.bert = BertModel.from_pretrained(modelpath\n","                                              ,num_labels=num_labels\n","                                              ,output_attentions=get_att\n","                                              ,output_hidden_states=get_hs)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(self.bert.config.hidden_size, num_labels)\n","        self.freeze_model = freeze_model\n","\n","        if self.freeze_model:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids\n","                            ,token_type_ids=None\n","                            ,attention_mask=attention_mask)\n","\n","        pooled_output = outputs.pooler_output\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.fc(pooled_output)\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPq566wnZ8kc"},"outputs":[],"source":["class CustomModel:\n","  def __init__(self, modelpath, class_weights):\n","    self.model = TextClassifier(modelpath, num_labels=NUM_LABELS, get_att=GET_ATTENTIONS, get_hs=GET_HIDDEN_STATES, dropout=DROPOUT)\n","\n","    self.optimizer = torch.optim.Adam(self.model.parameters()\n","                                      ,lr = LR\n","                                      ,eps = EPS\n","                                      ,weight_decay = WEIGHT_DECAY\n","                                      ,betas = BETAS # 0.9, 0.999\n","                                      ,amsgrad = AMSGRAD)\n","\n","    self.criterion = nn.CrossEntropyLoss(weight=class_weights # torch.tensor([5.8, 0.43, 2]).to(device)\n","                                         ,reduction = REDUCTION #sum\n","                                         ,label_smoothing = LABEL_SMOOTHING)\n","\n","  def freeze_all(self):\n","    #Freeze all layers except classifier\n","    for name, param in self.model.named_parameters():\n","      if \"encoder.layer\" in name:\n","        param.requires_grad = False\n","\n","  def unfreeze(self, layers=np.arange(0,13,1)):\n","    self.freeze_all()\n","    #Unfreeze specified layers\n","    for layer_no in layers:\n","      for name, param in self.model.named_parameters():\n","        if \"encoder.layer\" in name and str(layer_no) in name:\n","          param.requires_grad = True\n","\n","  def freeze_embs(self):\n","    for name, param in self.model.named_parameters():\n","      if \"embeddings\" in name:\n","        param.requires_grad = False\n","\n","  def validation(self, val_dataloader):\n","    t0 = time.time()\n","\n","    # We put the model in validation mode\n","    self.model.eval()\n","\n","    # We declare variables\n","    eval_loss = 0\n","    eval_metric = 0\n","    all_logits = []\n","    all_labels = []\n","\n","    # By minibatches\n","    for step, batch in enumerate(val_dataloader):\n","      b_input_ids, b_input_mask, b_labels = tuple(t.to(DEVICE) for t in batch)\n","\n","      with torch.no_grad():\n","        # We generate the predictions of the model\n","        outputs = self.model(b_input_ids,\n","                            attention_mask=b_input_mask)\n","\n","        loss = self.criterion(outputs, b_labels)\n","\n","        # ...we extract them\n","        logits = torch.argmax(outputs, dim=1).detach().cpu()\n","        b_labels = b_labels.to('cpu')\n","\n","        # Saving logits and labels. They will be useful for the confusion matrix.\n","        #predict_labels = np.argmax(logits, axis=1).flatten()\n","        all_logits.extend(logits.tolist())\n","        all_labels.extend(b_labels.tolist())\n","\n","        eval_loss += loss\n","\n","    # We calculate the F1 score of this batch\n","    scores = MulticlassF1Score(num_classes=2, average=None)(torch.tensor(all_logits), torch.tensor(all_labels))\n","    score = BinaryF1Score()(torch.tensor(all_logits), torch.tensor(all_labels))\n","    # We show the final accuracy for this epoch\n","    print(f\"\\n\\tF1Scores: {scores.tolist()}\")\n","    print(f\"\\n\\tEvalLoss: {eval_loss}\")\n","    print(f\"\\tValidation took: {format_time(time.time() - t0)}\")\n","    return scores, eval_loss\n","\n","\n","  def training(self, n_epochs, train_dataloader, val_dataloader, gradient_accumulator_size=2):\n","    max_step_t = len(train_dataloader)\n","    max_step_v = len(val_dataloader)\n","\n","    scheduler = torch.optim.lr_scheduler.PolynomialLR(self.optimizer\n","                                                      ,total_iters=n_epochs\n","                                                      ,power=POWER)\n","\n","    total_loss = []\n","    total_lr = []\n","\n","    for epoch in range(n_epochs):\n","      if epoch > 7:\n","        break\n","      # for each epoch...\n","      print(f\"\\nEpoch {epoch + 1} / {n_epochs} :\")\n","      # We save the start time to see how long it takes.\n","      t0 = time.time()\n","      # We reset the loss value for each epoch.\n","      epoch_loss = []\n","\n","      # Training mode.\n","      self.model.train()\n","      self.model.zero_grad()\n","\n","      for step, batch in enumerate(train_dataloader):\n","        batch_loss = 0\n","        b_input_ids, b_input_mask, b_labels = tuple(t.to(DEVICE) for t in batch)\n","\n","        # Propagation forward in the layers\n","        outputs = self.model(b_input_ids,\n","                        attention_mask=b_input_mask)\n","\n","        # We calculate the loss of the present minibatch\n","        loss = self.criterion(outputs, b_labels) #outputs[0]\n","        batch_loss += loss.item()\n","        epoch_loss.append( loss.item() )\n","\n","        # Backpropagation\n","        loss.backward()\n","\n","        # So we can implement gradien accumulator technique\n","        if (step > 0 and step % gradient_accumulator_size == 0) or (step == len(train_dataloader) - 1):\n","          #(this prevents the gradient from becoming explosive)\n","          torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n","\n","          # We update the weights and bias according to the optimizer\n","          self.optimizer.step()\n","          # We clean the gradients for the accumulator batch\n","          self.model.zero_grad()\n","\n","        b_input_ids.to(\"cpu\")\n","        b_input_mask.to(\"cpu\")\n","        b_labels.to(\"cpu\")\n","        del b_input_ids\n","        del b_input_mask\n","        del b_labels\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","        if step % 20 == 0 or step == max_step_t - 1:\n","          print(f\"Batch {step}/{max_step_t} avg loss: {np.sum(epoch_loss) / (step+1):.5f} {np.max(epoch_loss):.5f} {np.min(epoch_loss):.5f}\")\n","\n","      #Update learning rate each end of epoch\n","      scheduler.step()\n","      total_lr.append(scheduler.get_last_lr())\n","      total_loss.append(np.sum(epoch_loss)/len(train_dataloader))\n","\n","      # We calculate the average loss in the current epoch of the training set\n","      print(f\"\\n\\tAverage training loss: {np.sum(epoch_loss)/len(train_dataloader):.5f}\")\n","      print(f\"\\tTraining epoch took: {format_time(time.time() - t0)}\")\n","\n","      print(\"\\n\\tValidation\")\n","      curr_score, curr_eval_loss = self.validation(val_dataloader)\n","\n","    # Display learning rate and loss charts\n","    \"\"\"\n","    epoch_axis = np.arange(0, n_epochs) + 1\n","    plt.plot(epoch_axis, total_loss)\n","    plt.xlabel(\"epoch\")\n","    plt.ylabel(\"loss\")\n","    plt.xticks(epoch_axis)\n","    plt.show()\n","    \"\"\"\n","    \"\"\"\n","    print(\"\\n\\n\")\n","\n","    plt.plot(epoch_axis, total_lr)\n","    plt.xlabel(\"epoch\")\n","    plt.ylabel(\"learning rate\")\n","    plt.xticks(epoch_axis)\n","    plt.show()\n","    \"\"\"\n","    print(\"\\nTraining complete\")\n","\n","\n","  def testing(self, dataloader):\n","    preds = []\n","    labs = []\n","\n","    with torch.no_grad():\n","      for step, batch in enumerate(dataloader):\n","        test_inputs, test_masks, b_labels = tuple(t.to(DEVICE) for t in batch)\n","\n","        outputs = self.model(test_inputs,\n","                             attention_mask=test_masks)\n","\n","        logits = torch.argmax(outputs, dim=1).detach().cpu()\n","        b_labels = b_labels.to('cpu').tolist()\n","\n","        preds.extend([x.item() for x in logits])\n","        labs.extend(b_labels)\n","\n","    preds = [ \"P\" if pred == 0 else \"NP\" for pred in preds ]\n","    labs = [ str(l) + \"_Track3\" for l in labs ]\n","\n","    dataframe_res = { \"sub_id\": labs, \"label\": preds }\n","    dataframe_res = pd.DataFrame.from_dict(dataframe_res)\n","\n","    return dataframe_res\n","\n","  def metrics_testing(self, dataloader):\n","    preds = []\n","    labs = []\n","\n","    with torch.no_grad():\n","      for step, batch in enumerate(dataloader):\n","        test_inputs, test_masks, b_labels = tuple(t.to(DEVICE) for t in batch)\n","\n","        outputs = self.model(test_inputs,\n","                             attention_mask=test_masks)\n","\n","        logits = torch.argmax(outputs, dim=1).detach().cpu()\n","        b_labels = b_labels.to('cpu').tolist()\n","\n","        preds.extend([x.item() for x in logits])\n","        labs.extend(b_labels)\n","\n","    return preds, labs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibe3moZGu7i1"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","def report_metrics(data):\n","  preds, targets = data[0], data[1]\n","  print(classification_report(targets, preds, target_names=TARGET_NAMES, digits=4))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gUjJXYjqcZ4"},"outputs":[],"source":["import gc\n","\n","def fine_tuning_kfold(layers, desc, modelname, num_epochs = 10, embeddings = True, gradient_accumulator_size = 4):\n","  for kfold in datasets.dataloaders:\n","    if kfold > 2:\n","      break\n","\n","    train = datasets.dataloaders[kfold][\"train\"]\n","    dev = datasets.dataloaders[kfold][\"val\"]\n","    train_count = datasets.dfs[kfold][\"train\"][config.TEXT].count()\n","    dev_count = datasets.dfs[kfold][\"val\"][config.TEXT].count()\n","    print(f\"K-Fold: {kfold} -->  train size:{ train_count } val size:{ dev_count }\")\n","    print(\"------------------------\")\n","\n","    class_weights = get_class_weights([0,1], datasets.dfs[kfold][\"train\"][config.TARGET])\n","    print(f\"class weights: {class_weights}\")\n","    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n","    m = CustomModel(modelname, class_weights)\n","    m.unfreeze(layers)\n","\n","    if not embeddings:\n","      m.freeze_embs()\n","\n","    m.model.to(DEVICE)\n","    m.training(num_epochs, train, dev)\n","\n","    # m.model.save_pretrained(f\"{config.MODELPATH}{config.TRAINED_BETO}_{str(kfold)}\")\n","    print(\"Training complete. Validation results:\\n\")\n","    report_metrics(m.testing(dev))\n","\n","    m.model.to(\"cpu\")\n","    del m\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","#TODO : save best model and prepare test results\n","def fine_tuning(layers, desc, modelname, num_epochs = 10, embeddings = True, gradient_accumulator_size = 4):\n","  train = datasets.dataloaders[\"train\"]\n","  dev = datasets.dataloaders[\"dev\"]\n","  test = datasets.dataloaders[\"test\"]\n","\n","  train_count = datasets.dfs[\"train\"][config.TEXT].count()\n","  dev_count = datasets.dfs[\"dev\"][config.TEXT].count()\n","\n","  print(f\"train_size:{ train_count }   val_size:{ dev_count }\")\n","  print(\"------------------------\")\n","\n","  class_weights = get_class_weights([0,1], datasets.dfs[\"train\"][config.TARGET])\n","  class_weights[0] = class_weights[0] * 2\n","  print(f\"class weights: {class_weights}\")\n","  class_weights = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n","  m = CustomModel(modelname, class_weights)\n","  m.unfreeze(layers)\n","\n","  if not embeddings:\n","    m.freeze_embs()\n","\n","  m.model.to(DEVICE)\n","  m.training(num_epochs, train, dev)\n","  df_res = m.testing(test)\n","  df_res.to_csv(config.TRACK3 + \"tests/test_beto.csv\")\n","  # m.model.save_pretrained(f\"{config.MODELPATH}{config.TRAINED_BETO}_{str(kfold)}\")\n","  print(\"Training complete. Validation results:\\n\")\n","  report_metrics(m.metrics_testing(dev))\n","\n","  m.model.to(\"cpu\")\n","  del m\n","  torch.cuda.empty_cache()\n","  gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1770,"status":"ok","timestamp":1716837451046,"user":{"displayName":"Sergio Damian","userId":"14141393108500213286"},"user_tz":360},"id":"kuBdOqcl_870","outputId":"dbae2088-b071-4584-fdb9-7808715db7b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["...dataloader for train completed\n","...dataloader for dev completed\n","...dataloader for test completed\n"]}],"source":["#Obtener datasets\n","datasets = CustomDataset(config.TRACK3,\n","                         version = \"v6\",\n","                         batch_size = BATCH_SIZE,\n","                         modelname = config.C_BETO)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":139,"status":"ok","timestamp":1716837452309,"user":{"displayName":"Sergio Damian","userId":"14141393108500213286"},"user_tz":360},"id":"g72Qg_mezvBt","outputId":"1b16552b-8964-4254-e70c-1b4be42b6b40"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["label\n","1    904\n","0     39\n","Name: count, dtype: int64"]},"metadata":{},"execution_count":23}],"source":["datasets.dfs[\"train\"][config.TARGET].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YL7SvLt3JYmb","executionInfo":{"status":"ok","timestamp":1716838152380,"user_tz":360,"elapsed":698755,"user":{"displayName":"Sergio Damian","userId":"14141393108500213286"}},"outputId":"d10f7cf4-5dd2-4a7a-ce7e-dc655cc6f430"},"outputs":[{"output_type":"stream","name":"stdout","text":["train_size:943   val_size:600\n","------------------------\n","class weights: [24.17948718  0.5215708 ]\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 / 20 :\n","Batch 0/118 avg loss: 17.02647 17.02647 17.02647\n","Batch 20/118 avg loss: 8.54996 25.15743 3.05301\n","Batch 40/118 avg loss: 8.35806 44.23321 1.80532\n","Batch 60/118 avg loss: 9.30191 62.09171 0.97837\n","Batch 80/118 avg loss: 14.15950 103.15755 0.44690\n","Batch 100/118 avg loss: 13.70594 103.15755 0.26053\n","Batch 117/118 avg loss: 13.00067 103.15755 0.08690\n","\n","\tAverage training loss: 13.00067\n","\tTraining epoch took: 0:01:15\n","\n","\tValidation\n","\n","\tF1Scores: [0.0, 0.9655172228813171]\n","\n","\tEvalLoss: 3991.72998046875\n","\tValidation took: 0:00:10\n","\n","Epoch 2 / 20 :\n","Batch 0/118 avg loss: 90.97217 90.97217 90.97217\n","Batch 20/118 avg loss: 39.49754 195.44598 0.02472\n","Batch 40/118 avg loss: 40.39090 284.61258 0.00989\n","Batch 60/118 avg loss: 43.88051 305.06778 0.00837\n","Batch 80/118 avg loss: 53.91994 305.06778 0.00837\n","Batch 100/118 avg loss: 48.01802 305.06778 0.00837\n","Batch 117/118 avg loss: 43.40997 305.06778 0.00683\n","\n","\tAverage training loss: 43.40997\n","\tTraining epoch took: 0:01:15\n","\n","\tValidation\n","\n","\tF1Scores: [0.0, 0.9655172228813171]\n","\n","\tEvalLoss: 6085.66650390625\n","\tValidation took: 0:00:11\n","\n","Epoch 3 / 20 :\n","Batch 0/118 avg loss: 138.66531 138.66531 138.66531\n","Batch 20/118 avg loss: 55.95716 274.11832 0.00588\n","Batch 40/118 avg loss: 49.38231 308.53009 0.00588\n","Batch 60/118 avg loss: 48.91897 308.53009 0.00588\n","Batch 80/118 avg loss: 56.59796 308.53009 0.00588\n","Batch 100/118 avg loss: 49.92955 308.53009 0.00588\n","Batch 117/118 avg loss: 45.11715 308.53009 0.00443\n","\n","\tAverage training loss: 45.11715\n","\tTraining epoch took: 0:01:14\n","\n","\tValidation\n","\n","\tF1Scores: [0.0, 0.9655172228813171]\n","\n","\tEvalLoss: 6339.4912109375\n","\tValidation took: 0:00:11\n","\n","Epoch 4 / 20 :\n","Batch 0/118 avg loss: 143.76602 143.76602 143.76602\n","Batch 20/118 avg loss: 57.11753 274.36636 0.00388\n","Batch 40/118 avg loss: 50.37317 319.14325 0.00388\n","Batch 60/118 avg loss: 49.14795 319.14325 0.00388\n","Batch 80/118 avg loss: 56.59939 319.14325 0.00388\n","Batch 100/118 avg loss: 49.67257 319.14325 0.00388\n","Batch 117/118 avg loss: 45.00463 319.14325 0.00345\n","\n","\tAverage training loss: 45.00463\n","\tTraining epoch took: 0:01:14\n","\n","\tValidation\n","\n","\tF1Scores: [0.0, 0.9655172228813171]\n","\n","\tEvalLoss: 6305.1455078125\n","\tValidation took: 0:00:11\n","\n","Epoch 5 / 20 :\n","Batch 0/118 avg loss: 121.39027 121.39027 121.39027\n","Batch 20/118 avg loss: 56.50953 278.13287 0.00299\n","Batch 40/118 avg loss: 49.86922 325.01309 0.00299\n","Batch 60/118 avg loss: 48.51462 325.01309 0.00299\n","Batch 80/118 avg loss: 54.75664 325.01309 0.00299\n","Batch 100/118 avg loss: 47.53363 325.01309 0.00299\n","Batch 117/118 avg loss: 42.91788 325.01309 0.00292\n","\n","\tAverage training loss: 42.91788\n","\tTraining epoch took: 0:01:15\n","\n","\tValidation\n","\n","\tF1Scores: [0.0, 0.9655172228813171]\n","\n","\tEvalLoss: 6445.17041015625\n","\tValidation took: 0:00:11\n","\n","Epoch 6 / 20 :\n","Batch 0/118 avg loss: 119.88774 119.88774 119.88774\n","Batch 20/118 avg loss: 57.06672 287.96075 0.00271\n","Batch 40/118 avg loss: 50.49112 320.19620 0.00271\n","Batch 60/118 avg loss: 47.56272 320.19620 0.00271\n","Batch 80/118 avg loss: 52.63951 320.19620 0.00271\n","Batch 100/118 avg loss: 45.24463 320.19620 0.00271\n","Batch 117/118 avg loss: 40.88477 320.19620 0.00251\n","\n","\tAverage training loss: 40.88477\n","\tTraining epoch took: 0:01:15\n","\n","\tValidation\n","\n","\tF1Scores: [0.0, 0.9655172228813171]\n","\n","\tEvalLoss: 6362.2734375\n","\tValidation took: 0:00:11\n","\n","Epoch 7 / 20 :\n","Batch 0/118 avg loss: 81.56190 81.56190 81.56190\n","Batch 20/118 avg loss: 53.42723 264.36313 0.00238\n","Batch 40/118 avg loss: 48.36070 325.86618 0.00238\n","Batch 60/118 avg loss: 43.82272 325.86618 0.00238\n","Batch 80/118 avg loss: 49.13826 325.86618 0.00238\n","Batch 100/118 avg loss: 41.48315 325.86618 0.00238\n","Batch 117/118 avg loss: 37.39613 325.86618 0.00182\n","\n","\tAverage training loss: 37.39613\n","\tTraining epoch took: 0:01:14\n","\n","\tValidation\n","\n","\tF1Scores: [0.0, 0.9619377255439758]\n","\n","\tEvalLoss: 6564.5478515625\n","\tValidation took: 0:00:11\n","\n","Epoch 8 / 20 :\n","Batch 0/118 avg loss: 23.44584 23.44584 23.44584\n","Batch 20/118 avg loss: 52.39360 303.23630 0.00175\n","Batch 40/118 avg loss: 47.72210 331.54291 0.00175\n","Batch 60/118 avg loss: 40.83041 331.54291 0.00175\n","Batch 80/118 avg loss: 43.29010 331.54291 0.00175\n","Batch 100/118 avg loss: 36.46743 331.54291 0.00175\n","Batch 117/118 avg loss: 32.82311 331.54291 0.00175\n","\n","\tAverage training loss: 32.82311\n","\tTraining epoch took: 0:01:15\n","\n","\tValidation\n","\n","\tF1Scores: [0.03846153989434242, 0.9564459919929504]\n","\n","\tEvalLoss: 6354.9404296875\n","\tValidation took: 0:00:11\n","\n","Training complete\n","Training complete. Validation results:\n","\n","              precision    recall  f1-score   support\n","\n","           P     0.0833    0.0250    0.0385        40\n","          NP     0.9337    0.9804    0.9564       560\n","\n","    accuracy                         0.9167       600\n","   macro avg     0.5085    0.5027    0.4975       600\n","weighted avg     0.8770    0.9167    0.8952       600\n","\n"]}],"source":["layers = [str(x) for x in range(0, 12)]\n","fine_tuning(layers, \"all_layers_cased\", config.C_BETO, num_epochs=NUM_EPOCHS + 10, gradient_accumulator_size=GRADIENT_ACCUMULATOR_SIZE)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}